{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3a4258-9608-4feb-9a59-b4716fddd502",
   "metadata": {},
   "source": [
    "# Bayesian Analysis of Math Exam Scores\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You are provided with a dataset of math exam scores from Estonian primary schools. Your task is to build a Bayesian model to better understand the factors that influence exam performance.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The dataset includes the following variables:\n",
    "- `year`: The year of the exam (2016-2023), with [2020 missing due to the COVID-19 pandemic](https://www.ohtuleht.ee/998159/pohikooli-eksamid-jaavad-ara-riigieksamid-toimuvad)\n",
    "- `county`: The county where the student took the exam\n",
    "- `domestic_background`: Whether the student is Estonian or Russian\n",
    "- `gender`: Student's gender (Male/Female)\n",
    "- `avg_grade`: The student's average grade (from 1 to 5, continuous)\n",
    "- `exam_score`: The exam score (0-50 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8894769c-2d15-45f4-9dc7-e8e3c2a277f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries, import and manipulate \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "#df = pd.read_csv('math_exam_scores.csv', sep=\";\").sample(4000) # Work with a smaller dataset at first so that you don't have to wait 10 minutes every time you sample the model.\n",
    "df = pd.read_csv('math_exam_scores.csv', sep=\";\") \n",
    "\n",
    "# DATA MANIPULATIONS #\n",
    "## YEAR ##\n",
    "# Convert year to category as there are probably non-linear effects at play\n",
    "df['year'] = df['year'].astype('category')\n",
    "\n",
    "# Encode categorical variable 'year' as integers for indexing\n",
    "df['year'] = df['year'].cat.codes\n",
    "n_years = df['year'].nunique()\n",
    "\n",
    "## COUNTY ##\n",
    "# Make sure county is a category\n",
    "df['county'] = df['county'].astype('category')\n",
    "\n",
    "# Encode categorical variable 'county'\n",
    "df['county'] = df['county'].cat.codes\n",
    "n_counties = df['county'].nunique()\n",
    "\n",
    "## GENDER ##\n",
    "# Convert domestic_background to numerical binary\n",
    "remap = {'Male': 0, 'Female': 1}\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "df['gender'] = df['gender'].replace(remap).values.astype(int)\n",
    "\n",
    "## DOMESTIC BACKGROUND ##\n",
    "# Convert domestic_background to numerical binary\n",
    "remap = {'Estonian': 0, 'Russian': 1}\n",
    "df['domestic_background'] = df['domestic_background'].replace(remap).values.astype(int)\n",
    "\n",
    "## CONTINIOUS VARIABLES ##\n",
    "# Rescaling exam score and since the Beta distribution is the most intuitive choice, then make sure that\n",
    "#  the rescled exam score is strictly within open interval (0,1), ie. cannot be [0,1].\n",
    "eps = 1e-6\n",
    "df['exam_score_rescaled'] = (df['exam_score'] / 50.0).clip(eps, 1-eps)\n",
    "\n",
    "# Rescaling avg_grade\n",
    "df['avg_grade_rescaled'] = df['avg_grade'] / 5.0\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a229c-a2e3-4f9f-b3d1-ae462e7c7ce5",
   "metadata": {},
   "source": [
    "## Problem analysis\n",
    "\n",
    "The DAG that we'll take as the basis of our analysis.\n",
    "\n",
    "<img src=\"DAG.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "### Justification for the DAG\n",
    "\n",
    "**Year** - Since this standardised matematics exam is administered to the whole country simultaneously in set year, then all of the possible influencing factors are captured in this factor. Exam exercises are changed every year, so [some years tend to be easier](https://www.err.ee/1609007237/pohikooli-matemaatikaeksami-sooritused-olid-paremad-kui-kahel-eelneval-aastal) and some harder, there are global effects that come into play (eg. COVID-19 pandemic), policy changes, cohort effects etc.<br>\n",
    "Since all other variables result from a progression of longer processes (eg. county effects on exam score) and any shift in them cannot be attributed to a single year, then Year of the exam does not have influence on other factors.\n",
    "\n",
    "**County** - Differences in school education quality, population parameters (eg. urban/rural porportions), socio/economic factors are encompassed into the county variable. For that reason county is set to influence academic ability and average grade. <br>\n",
    "The distribution of Estonian and Russian speaking families is different between counties. Therefore count -> domestic_background. \n",
    "\n",
    "**Domestic background** - [PISA statistics](https://harno.ee/sites/default/files/documents/2023-12/Pisa_tulemused_2022_veebi.pdf) show that Estonian and Russian students' academic ability differs quite a lot in Estonia.\n",
    "\n",
    "**Gender** - [PISA statistics](https://harno.ee/sites/default/files/documents/2023-12/Pisa_tulemused_2022_veebi.pdf) show that there is a gap in academic ability between boys and girls in Estonia.\n",
    "\n",
    "**Academic ability** - Exam score is meant to measure academic ability of a student, but its an approximation. Its taken as a latent variable into the model. Academic ability is influenced by multiple factors such as school education quality, environment (mediated by county), domestic background and gender etc. <br> \n",
    "Average grade also tries to measure academic ability, but has other influencing factors at play (eg. group level effects of school and county, girls tend to worry more about grades than boys). So it cannot be taken as the same kind of measure of academic ability as the exam score.\n",
    "\n",
    "**Average grade** - Average grade is another attempt at measuring academic ability, but is less standardised. It is also dependent on group level effects from schools and thus counties, ie. a '4' in Harjumaa is likely not the same as a '4' in Võrumaa. Since its a an outcome of academic ability, then it is highly correlated with exam score. <br>\n",
    "Average grade does however influences **the variability of exam score**, ie. students who have average grade near 5 are probably very good at exams and tests. Knowledge enables them to be faster, thus having more time to overlook the exam submission, finding typos and small mistakes. On the opposite end of the spectrum students who have a very low average grade will probably also struggle on the exam, so its vey unlikely they will suddenly score much points on the exam, therefore variability is also lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c29a00-7eee-488a-a6f8-289d2b30ca9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n",
       " <span style=\"font-weight: bold\"> Progress                  </span> <span style=\"font-weight: bold\"> Draws </span> <span style=\"font-weight: bold\"> Divergences </span> <span style=\"font-weight: bold\"> Step size </span> <span style=\"font-weight: bold\"> Grad evals </span> <span style=\"font-weight: bold\"> Sampling Speed </span> <span style=\"font-weight: bold\"> Elapsed </span> <span style=\"font-weight: bold\"> Remaining </span> \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━</span>   208     0             0.00        1023         6.28 s/draws     0:21:59   12:36:25   \n",
       "  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━</span>   211     0             0.01        1023         6.14 s/draws     0:21:59   12:39:34   \n",
       "  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━</span>   214     0             0.01        511          6.16 s/draws     0:21:59   6:35:18    \n",
       "  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━</span>   211     0             0.01        511          6.23 s/draws     0:21:59   6:18:59    \n",
       "                                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                                   \n",
       " \u001b[1m \u001b[0m\u001b[1mProgress                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDraws\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDivergences\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mStep size\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGrad evals\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSampling Speed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mElapsed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mRemaining\u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;31;119;180m━━\u001b[0m\u001b[38;2;31;119;180m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   208     0             0.00        1023         6.28 s/draws     0:21:59   12:36:25   \n",
       "  \u001b[38;2;31;119;180m━━\u001b[0m\u001b[38;2;31;119;180m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   211     0             0.01        1023         6.14 s/draws     0:21:59   12:39:34   \n",
       "  \u001b[38;2;31;119;180m━━\u001b[0m\u001b[38;2;31;119;180m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   214     0             0.01        511          6.16 s/draws     0:21:59   6:35:18    \n",
       "  \u001b[38;2;31;119;180m━━\u001b[0m\u001b[38;2;31;119;180m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   211     0             0.01        511          6.23 s/draws     0:21:59   6:18:59    \n",
       "                                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m y_obs = pm.Beta(\u001b[33m\"\u001b[39m\u001b[33my_obs\u001b[39m\u001b[33m\"\u001b[39m, alpha=phi*kappa, beta=(\u001b[32m1\u001b[39m-phi)*kappa, observed=df.exam_score_rescaled)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Sampling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m trace = \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m pm.sample_posterior_predictive(trace, extend_inferencedata=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     35\u001b[39m pm.compute_log_likelihood(trace, extend_inferencedata=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bayes2025_backup/lib/python3.12/site-packages/pymc/sampling/mcmc.py:964\u001b[39m, in \u001b[36msample\u001b[39m\u001b[34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, compile_kwargs, **kwargs)\u001b[39m\n\u001b[32m    960\u001b[39m t_sampling = time.time() - t_start\n\u001b[32m    962\u001b[39m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[32m    963\u001b[39m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZarrTrace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bayes2025_backup/lib/python3.12/site-packages/pymc/sampling/mcmc.py:1049\u001b[39m, in \u001b[36m_sample_return\u001b[39m\u001b[34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     traces, length = \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1051\u001b[39m     traces, length = _choose_chains(traces, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bayes2025_backup/lib/python3.12/site-packages/pymc/backends/base.py:624\u001b[39m, in \u001b[36m_choose_chains\u001b[39m\u001b[34m(traces, tune)\u001b[39m\n\u001b[32m    622\u001b[39m lengths = [\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) - tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot enough samples to build a trace.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    626\u001b[39m idxs = np.argsort(lengths)\n\u001b[32m    627\u001b[39m l_sort = np.array(lengths)[idxs]\n",
      "\u001b[31mValueError\u001b[39m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "with pm.Model() as model_beta_heterosc4:    \n",
    "    # Priors to modeling academic_ability\n",
    "    acad_intercept = pm.Normal(\"acad_intercept\", mu=0, sigma=1)\n",
    "    beta_gender = pm.Normal(\"beta_gender\", mu=0, sigma=1)\n",
    "    beta_domestic = pm.Normal(\"beta_domestic\", mu=0, sigma=1)\n",
    "    beta_county = pm.Normal(\"beta_county\", mu=0, sigma=1, shape=n_counties)\n",
    "    \n",
    "    # Latent variable: academic ability\n",
    "    mu = (acad_intercept\n",
    "        + beta_gender * df.gender\n",
    "        + beta_domestic * df.domestic_background\n",
    "        + beta_county[df.county])\n",
    "    academic_ability = pm.Normal(\"academic_ability\", mu=mu, sigma=1)\n",
    "\n",
    "    # Priors to modeling exam score\n",
    "    phi_intercept = pm.Normal(\"phi_intercept\", mu=0, sigma=1)\n",
    "    kappa_intercept = pm.Normal(\"kappa_intercept\", mu=0, sigma=1)\n",
    "    beta_academic = pm.Normal(\"beta_academic\", mu=0, sigma=1)\n",
    "    beta_year = pm.Normal(\"beta_year\", mu=0, sigma=1, shape=n_years)\n",
    "    beta_grade1 = pm.Normal(\"beta_grade1\", mu=0, sigma=1)\n",
    "    beta_grade2 = pm.Normal(\"beta_grade2\", mu=0, sigma=1)\n",
    "\n",
    "    # Based on https://distribution-explorer.github.io/continuous/beta.html trying to reparametrize with phi and kappa \n",
    "    # Where phi is the mean and kappa is variance\n",
    "    phi = pm.math.sigmoid(phi_intercept + beta_academic*academic_ability + beta_grade1*df.avg_grade + beta_year[df.year]) + eps\n",
    "    kappa = kappa_intercept + beta_grade2 * df.avg_grade\n",
    "    \n",
    "    y_obs = pm.Beta(\"y_obs\", alpha=phi*kappa, beta=(1-phi)*kappa, observed=df.exam_score_rescaled)\n",
    "        \n",
    "    # Sampling\n",
    "    trace = pm.sample(1000, tune=1000, target_accept=0.9, return_inferencedata=True)\n",
    "    pm.sample_posterior_predictive(trace, extend_inferencedata=True)\n",
    "    pm.compute_log_likelihood(trace, extend_inferencedata=True)\n",
    "    models['model_beta_heterosc4'] = trace\n",
    "\n",
    "# Summary\n",
    "var_names=[c for c in list(trace.posterior.data_vars) if not(c.startswith('academic_ability') or c.startswith('heteroscedacity_component'))]\n",
    "az.summary(trace, round_to=2, var_names=var_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623f67f-b5b6-4d45-87d3-7c5615b44e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model diagnostics\n",
    "print(pm.summary(trace))\n",
    "pm.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf89702-7092-42d5-8c64-1dd53355ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_ppc(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900ee38-ba68-4478-bd29-b40df6c788a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_ess(trace, kind=\"evolution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
